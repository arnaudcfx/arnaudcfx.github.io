<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>High Performance Scientific Computing</title>
  <meta name="viewport" content="width=device-width,initial-scale=1">
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;600;700&display=swap" rel="stylesheet">
  <style>
    :root {
      --violet: #7c3aed;
      --violet-dark: #5b21b6;
      --violet-light: #ede9fe;
      --gray: #6b7280;
      --dark: #1e1b4b;
      --light: #fafafa;
      --radius: 12px;
      --shadow: 0 4px 8px rgba(0,0,0,0.1);
    }
    body {
      font-family: 'Inter', sans-serif;
      background: linear-gradient(135deg, #f5f3ff 0%, #ede9fe 100%);
      margin: 0;
      color: var(--dark);
      line-height: 1.7;
    }
    .container {
      max-width: 1000px;
      margin: 3rem auto;
      background: white;
      padding: 3rem;
      border-radius: var(--radius);
      box-shadow: var(--shadow);
    }
    h1 {
      color: var(--violet);
      font-size: 2.3rem;
      text-align: center;
      margin-bottom: 0.5rem;
    }
    h2 {
      color: var(--violet-dark);
      border-left: 4px solid var(--violet);
      padding-left: 0.6rem;
      margin-top: 2.5rem;
    }
    p, li {
      color: var(--gray);
      font-size: 1.05rem;
    }
    .team {
      text-align: center;
      margin-bottom: 2rem;
      color: var(--gray);
      font-weight: 500;
    }
    .lang-list {
      display: flex;
      justify-content: center;
      flex-wrap: wrap;
      gap: 0.5rem;
      margin: 1.5rem 0 2rem;
    }
    .lang-tag {
      background: var(--violet-light);
      color: var(--violet-dark);
      border-radius: 20px;
      padding: 6px 14px;
      font-weight: 600;
      font-size: 0.9rem;
    }
    .highlight {
      background: var(--violet-light);
      padding: 0.3rem 0.6rem;
      border-radius: 6px;
      font-weight: 500;
      color: var(--violet-dark);
    }
    footer {
      text-align: center;
      margin-top: 3rem;
      color: var(--gray);
      font-size: 0.9rem;
    }
    .notice { background: #fff3cd; color: #856404; border-radius: 6px; padding: 1em 1.2em; margin-top: 2em; font-size: 1.05em; border-left: 5px solid #ffe066; }
  </style>
</head>
<body>
  <div class="container">
    <h1>High Performance Scientific Computing</h1>
    <p class="team">Team: Arnaud Crucifix & Samuel Daoud</p>

    <div class="lang-list">
      <span class="lang-tag">C</span>
      <span class="lang-tag">MPI</span>
      <span class="lang-tag">OpenMP</span>
      <span class="lang-tag">Parallel Computing</span>
      <span class="lang-tag">HPC</span>
    </div>

    <h2>Project Overview</h2>
    <p>
      This project focused on developing and optimizing a <strong>Finite-Difference Time-Domain (FDTD)</strong> simulation for wave propagation using <span class="highlight">parallel computing</span> techniques.
      We implemented and compared <strong>OpenMP</strong> (shared memory) and <strong>MPI</strong> (distributed memory) versions to analyze performance, scalability, and communication efficiency.
    </p>

    <h2>Core Algorithm</h2>
    <p>
      The simulation updates <strong>pressure (P)</strong> and <strong>velocity (Vx, Vy, Vz)</strong> values over time and space.  
      Each iteration performs:
    </p>
    <ul>
      <li>Pressure update based on velocity divergence.</li>
      <li>Velocity update based on pressure gradient.</li>
      <li>Enforcement of boundary conditions and source terms.</li>
    </ul>
    <p>
      The computational complexity is <strong>O(T × Nᴰ)</strong>, where <em>T</em> is time steps, <em>N</em> is spatial size, and <em>D</em> the number of dimensions.
    </p>

    <h2>Parallelization Strategy</h2>
    <p>
      The domain was <strong>split across processes</strong> along one dimension.  
      Each process updated its local subdomain and exchanged <span class="highlight">boundary data</span> with its neighbors.  
      This required minimal communication overhead thanks to <strong>efficient halo exchange</strong> logic.
    </p>
    <ul>
      <li><strong>OpenMP:</strong> Shared-memory parallelization; results aggregated automatically.</li>
      <li><strong>MPI:</strong> Distributed-memory approach; each process sends and receives border columns.</li>
    </ul>

    <h2>Results</h2>
    <h3>OpenMP Strong Scaling</h3>
    <p>Speedup achieved across cores:</p>
    <ul>
      <li>2 cores → ×1.45</li>
      <li>4 cores → ×2.32</li>
      <li>8 cores → ×3.02</li>
      <li>16 cores → ×3.65</li>
      <li>32 cores → ×3.64</li>
    </ul>

    <h3>MPI Strong Scaling</h3>
    <ul>
      <li>2 processes → ×1.52</li>
      <li>4 processes → ×2.27</li>
      <li>8 processes → ×2.67</li>
      <li>16 processes → ×2.88</li>
      <li>32 processes → ×2.73</li>
    </ul>

    <h3>MPI Weak Scaling</h3>
    <ul>
      <li>4 processes → 58s</li>
      <li>16 processes → 132.6s</li>
      <li>32 processes → 248.5s</li>
    </ul>
    <p>Performance degradation was attributed to <strong>bandwidth limitations</strong> as communication volume increased.</p>

    <div class="notice">
      The full source code is not published to respect the academic and group work confidentiality.
    </div>
    
    <footer>
      <p>© 2025 | High Performance Scientific Computing </p>
    </footer>
  </div>
</body>
</html>

